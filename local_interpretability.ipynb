{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shelve\n",
    "import marisa_trie\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from med_lab_autocomplete_utils import LabAutocomplete, MedAutocomplete\n",
    "from symptom_utils.symptom_autocomplete_utils import SymptomAutocomplete\n",
    "from hpi_utils.hpi_autocomplete_utils import HPIAutocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_autocomplete = LabAutocomplete()\n",
    "medication_autocomplete = MedAutocomplete()\n",
    "symptom_autocomplete = SymptomAutocomplete()\n",
    "hpi_autocomplete = HPIAutocomplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_FP = '' # fill in here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading ED and OMR data...')\n",
    "ed_visits_pkl = pickle.load(open(f'{DATA_BASE_FP}/ed_data/visits_full.pkl'))\n",
    "omr_data = shelve.open(f'{DATA_BASE_FP}/ed_data/omr/omrShelfPatient_py3_jclinic')\n",
    "with open(f'{DATA_BASE_FP}/jclinic/extracted_data/ed_chief_complaints.pkl', 'rb') as h:\n",
    "    ed_chief_complaints = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_BASE_FP}/jclinic/extracted_data/allowable_umls_lookups.pkl', 'r') as h:\n",
    "    allowable_umls_terms = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_BASE_FP}/jclinic/extracted_data/umls_to_history_bucket_v2.pkl', 'r') as h:\n",
    "    umls_to_hist_bucket = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_ontology = pd.read_csv('ontologies/hpi_autocomplete_ontology.csv', index_col=0)\n",
    "hpi_ontology['synonyms'] = hpi_ontology['synonyms'].apply(pd.eval)\n",
    "symptom_ontology = pd.read_csv('ontologies/symptom_autocomplete_ontology.csv')\n",
    "symptom_ontology['synonyms'] = symptom_ontology['synonyms'].apply(pd.eval)\n",
    "with open('ontologies/medication_ontology.json', 'r') as h:\n",
    "    med_list = json.load(h)['freq']\n",
    "with open('ontologies/lab_ontology.json', 'r') as h:\n",
    "    lab_list = json.load(h)['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie_keys = set()\n",
    "term_lookup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, med in enumerate(med_list):\n",
    "    trie_keys.add(med)\n",
    "    term_lookup[med] = ('MEDICATION', i)\n",
    "for i, syns in enumerate(hpi_ontology['synonyms']):\n",
    "    if hpi_ontology.loc[i]['ignore']:\n",
    "        continue\n",
    "    for s in syns:\n",
    "        trie_keys.add(s)\n",
    "        term_lookup[s] = ('DISEASE', i)\n",
    "for i, syns in enumerate(symptom_ontology['synonyms']):\n",
    "    if symptom_ontology.loc[i]['ignore']:\n",
    "        continue\n",
    "    for s in syns:\n",
    "        trie_keys.add(s)\n",
    "        term_lookup[s] = ('SYMPTOM', i)\n",
    "for i, lab in enumerate(lab_list):\n",
    "    trie_keys.add(lab)\n",
    "    term_lookup[lab] = ('LAB', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_trie = marisa_trie.Trie(trie_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synonyms = {\n",
    "    'DISEASE' : list(hpi_ontology['synonyms']), \n",
    "    'SYMPTOM' : list(symptom_ontology['synonyms']), \n",
    "    'LAB' : [[lab] for lab in lab_list],\n",
    "    'MEDICATION' : [[med] for med in med_list]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concepts(tokens):\n",
    "    concepts = {}\n",
    "    blacklisted_toks = set()\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if i in blacklisted_toks:\n",
    "            continue\n",
    "        future_txt = ''.join(tokens[i:])\n",
    "        potentials = search_trie.prefixes(unicode(future_txt))\n",
    "        if not potentials:\n",
    "            continue\n",
    "        best = max(potentials, key=len)\n",
    "        if len(future_txt) == len(best) or future_txt[len(best)] in ' ,;:.':\n",
    "            concept = term_lookup[best][0]\n",
    "            concepts[i] = (concept, best)\n",
    "            for j in range(len(re.split('(\\s|[\\.!\\?,;])', best))):\n",
    "                blacklisted_toks.add(j + i)\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.random.seed(0)\n",
    "test_indices = np.random.choice(len(ed_visits_pkl), 25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logic to detect scope and type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_omr(visit_ix):\n",
    "    visit = ed_visits_pkl[visit_ix]\n",
    "    md_comments = visit['MDcomments'][0]\n",
    "    visit_date = datetime.strptime(visit['Date'][0][14:24], '%Y-%m-%d')\n",
    "    pid = visit['PatientID'][0]\n",
    "    if not md_comments:\n",
    "        return None\n",
    "    triage_assessment = visit['TriageAssessment'][0]\n",
    "    omr_buckets, omr_terms = [], []\n",
    "    if pid in omr_data:\n",
    "        last_omr_note = 0\n",
    "        for i, note in enumerate(omr_data[pid]):\n",
    "            if note['time'] < visit_date:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omr_date_parser = lambda x : datetime.strptime(x['time'], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def generate_lime_data(visit_ix, verbose=False):\n",
    "    visit = ed_visits_pkl[visit_ix]\n",
    "    md_comments = visit['MDcomments'][0]\n",
    "    visit_date = datetime.strptime(visit['Date'][0][14:24], '%Y-%m-%d')\n",
    "    pid = visit['PatientID'][0]\n",
    "    triage_assessment = visit['TriageAssessment'][0]\n",
    "    if not md_comments or not triage_assessment:\n",
    "        return None\n",
    "    hpi = md_comments.split('\\n')[0].lower()\n",
    "    if verbose:\n",
    "        print(hpi)\n",
    "    tokens = [t for t in re.split('(\\s|[\\.!\\?,;])', hpi) if t != '']\n",
    "    concepts = find_concepts(tokens)\n",
    "    omr_buckets, omr_terms = [], []\n",
    "    if pid in omr_data:\n",
    "        last_omr_note = 0\n",
    "        for i, note in enumerate(omr_data[pid]):\n",
    "            if omr_date_parser(note) >= visit_date:\n",
    "                last_omr_note = i\n",
    "                break\n",
    "        omr_notes = [] if (last_omr_note==0) else omr_data[pid][:i]\n",
    "        omr_buckets, omr_terms = hpi_autocomplete.get_omr_buckets(omr_notes)\n",
    "    if verbose:\n",
    "        print(omr_terms)\n",
    "        print(triage_assessment, list(omr_buckets))\n",
    "    X1, X2, y = hpi_autocomplete.get_lime_data(triage_assessment, list(omr_buckets))\n",
    "    return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lime_data = [generate_lime_data(_) for _ in range(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_data = [l for l in lime_data if l is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([np.squeeze(x1) for x1, x2, y in lime_data])\n",
    "X2 = np.array([np.squeeze(x2) for x1, x2, y in lime_data])\n",
    "Y = np.array([y for x1, x2, y in lime_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = Lasso(alpha=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_index = 1\n",
    "reg_model = reg_model.fit(np.hstack((X1, X2)), np.log(Y[:, diabetes_index]/(1-Y[:, diabetes_index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cats = {i : word for word, i in hpi_autocomplete.triage_vectorizer.vocabulary_.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
