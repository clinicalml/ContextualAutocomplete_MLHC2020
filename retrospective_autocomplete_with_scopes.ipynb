{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shelve\n",
    "import marisa_trie\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load rankers for labs, meds, symptoms, and conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpi_utils\n",
    "reload(hpi_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from med_lab_autocomplete_utils import LabAutocomplete, MedAutocomplete\n",
    "from symptom_utils.symptom_autocomplete_utils import SymptomAutocomplete\n",
    "from hpi_utils.hpi_autocomplete_utils import HPIAutocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_autocomplete = LabAutocomplete()\n",
    "medication_autocomplete = MedAutocomplete()\n",
    "symptom_autocomplete = SymptomAutocomplete()\n",
    "hpi_autocomplete = HPIAutocomplete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "* ED data contains triage information and notes written in the emergency department, per patient (shuffled in a random order).\n",
    "* OMR/EHR data refers to all prior clinical notes in a patient's record (keyed by PatientID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Loading ED and OMR data...')\n",
    "ed_visits_pkl = pickle.load(open('/data/BIDMC/ed_data/visits_full.pkl'))\n",
    "omr_data = shelve.open('/data/BIDMC/ed_data/omr/omrShelfPatient_py3_jclinic')\n",
    "with open('/data/BIDMC/jclinic/extracted_data/ed_chief_complaints.pkl', 'rb') as h:\n",
    "    ed_chief_complaints = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open ontologies and create trie-based datastructure to find clinical concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_ontology = pd.read_csv('ontologies/hpi_autocomplete_ontology.csv', index_col=0)\n",
    "hpi_ontology['synonyms'] = hpi_ontology['synonyms'].apply(pd.eval)\n",
    "symptom_ontology = pd.read_csv('ontologies/symptom_autocomplete_ontology.csv')\n",
    "symptom_ontology['synonyms'] = symptom_ontology['synonyms'].apply(pd.eval)\n",
    "with open('ontologies/medication_ontology.json', 'r') as h:\n",
    "    med_list = json.load(h)['freq']\n",
    "with open('ontologies/lab_ontology.json', 'r') as h:\n",
    "    lab_list = json.load(h)['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie_keys = set()\n",
    "term_lookup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, med in enumerate(med_list):\n",
    "    trie_keys.add(med)\n",
    "    term_lookup[med] = ('MEDICATION', i)\n",
    "for i, syns in enumerate(hpi_ontology['synonyms']):\n",
    "    if hpi_ontology.loc[i]['ignore']:\n",
    "        continue\n",
    "    for s in syns:\n",
    "        if s in ['as', 'ks', 'uc', 'rd', 'vt', 'di']: # cannot be disambiguated:\n",
    "            continue\n",
    "        trie_keys.add(s)\n",
    "        term_lookup[s] = ('DISEASE', i)\n",
    "for i, syns in enumerate(symptom_ontology['synonyms']):\n",
    "    if symptom_ontology.loc[i]['ignore']:\n",
    "        continue\n",
    "    for s in syns:\n",
    "        trie_keys.add(s)\n",
    "        term_lookup[s] = ('SYMPTOM', i)\n",
    "for i, lab in enumerate(lab_list):\n",
    "    trie_keys.add(lab)\n",
    "    term_lookup[lab] = ('LAB', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_trie = marisa_trie.Trie(trie_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_synonyms = {\n",
    "    'DISEASE' : list(hpi_ontology['synonyms']), \n",
    "    'SYMPTOM' : list(symptom_ontology['synonyms']), \n",
    "    'LAB' : [[lab] for lab in lab_list],\n",
    "    'MEDICATION' : [[med] for med in med_list]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find clinical concepts retrospectively in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concepts(tokens):\n",
    "    concepts = {}\n",
    "    blacklisted_toks = set()\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if i in blacklisted_toks:\n",
    "            continue\n",
    "        future_txt = ''.join(tokens[i:])\n",
    "        potentials = search_trie.prefixes(unicode(future_txt))\n",
    "        if not potentials:\n",
    "            continue\n",
    "        best = max(potentials, key=len)\n",
    "        if len(future_txt) == len(best) or future_txt[len(best)] in ' ,;:.':\n",
    "            concept = term_lookup[best][0]\n",
    "            concepts[i] = (concept, best)\n",
    "            for j in range(len(re.split('(\\s|[\\.!\\?,;])', best))):\n",
    "                blacklisted_toks.add(j + i)\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure MRR, MAPK, and keystroke burden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ranking(suggested, term): \n",
    "    suggested.remove(term)\n",
    "    suggested.append(term)\n",
    "    return suggested\n",
    "\n",
    "def get_num_keystrokes_singlequery(suggested_ranking, relevant_term, relevant_text, k=1):\n",
    "    # relevant terms are now (TERM_TYPE, index) \n",
    "    def word_in_set(word, suggested_term):\n",
    "        suggested_term_type, suggested_term_index = suggested_term\n",
    "        syns = all_synonyms[suggested_term_type][suggested_term_index]\n",
    "        for s in syns:\n",
    "            if word in s:\n",
    "                return True\n",
    "        return False\n",
    "    for i in range(len(relevant_text)):\n",
    "        word = relevant_text[:i]\n",
    "        new_suggested = [s for s in suggested_ranking if word_in_set(word, s)]\n",
    "        if relevant_term in new_suggested[:min(k, len(new_suggested))]:\n",
    "            return i\n",
    "    return len(relevant_text)\n",
    "\n",
    "def reciprocal_rank(suggested_ranking, relevant_term, num_relevant):\n",
    "    return (1.0/float(suggested_ranking.index(relevant_term) - num_relevant + 2) if suggested_ranking.index(relevant_term) >= num_relevant else 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.random.seed(0) # fix random seed so train/test split is preserved \n",
    "test_indices = np.random.choice(len(ed_visits_pkl), 25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logic to detect autocomplete scope and type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trigger set and what concept type it maps to\n",
    "scope_trigger_types = { \n",
    "    'p/w' : 'SYMPTOM',\n",
    "    'presents with' : 'SYMPTOM',\n",
    "    'presented with' : 'SYMPTOM',\n",
    "    'presenting with' : 'SYMPTOM',\n",
    "    'presents w/' : 'SYMPTOM',\n",
    "    'presented w/' : 'SYMPTOM',\n",
    "    'presenting w/' : 'SYMPTOM',\n",
    "    'came in with' : 'SYMPTOM',\n",
    "    'c/o' : 'SYMPTOM',\n",
    "    'complains of' : 'SYMPTOM',\n",
    "    'complained of' : 'SYMPTOM',\n",
    "    'complaining of' : 'SYMPTOM',\n",
    "    's/p' : 'SYMPTOM',\n",
    "    'status post' : 'SYMPTOM',\n",
    "    'h/o' : 'DISEASE',\n",
    "    'hx of' : 'DISEASE',\n",
    "    'pmh' : 'DISEASE',\n",
    "    'history of' : 'DISEASE',\n",
    "    'on' : 'MEDICATION',\n",
    "    'had' : 'SYMPTOM',\n",
    "    'has' : 'SYMPTOM',\n",
    "    'had r' : 'SYMPTOM',\n",
    "    'had l' : 'SYMPTOM',\n",
    "    'had right' : 'SYMPTOM',\n",
    "    'had left' : 'SYMPTOM',\n",
    "    'but no' : 'SYMPTOM',\n",
    "    'onset of' : 'SYMPTOM',\n",
    "    'describes' : 'SYMPTOM',\n",
    "    'describes having' : 'SYMPTOM',\n",
    "    'denies' : 'SYMPTOM',\n",
    "    'notes' : 'SYMPTOM',\n",
    "    'diagnosed with' : 'DISEASE',\n",
    "    'has' : 'DISEASE',\n",
    "    'felt like' : 'SYMPTOM', \n",
    "    'takes' : 'MEDICATION',\n",
    "    'treated with' : 'MEDICATION',\n",
    "    'with' : 'SYMPTOM'\n",
    "}\n",
    "\n",
    "# enumerate each concept type \n",
    "term_trigger_types = {\n",
    "    0 : 'DISEASE',\n",
    "    1 : 'SYMPTOM',\n",
    "    2 : 'MEDICATION', \n",
    "    3 : 'LAB'\n",
    "}\n",
    "\n",
    "scope_trigger_trie = marisa_trie.Trie(scope_trigger_types.keys())\n",
    "\n",
    "# continuation tokens do not affect scope \n",
    "scope_continuation_tokens = ['and', 'any', 'or', 'no', 'of', 'with', 'but', ',', '\"', 'abd', 'l', 'r', 'left', 'right']\n",
    "\n",
    "def predict_autocomplete_scope(tokens, is_structured):\n",
    "    \"\"\"\n",
    "    Greedily determines how to autocomplete from a set of tokens. \n",
    "    \n",
    "    Note that tokens include whitespace and punctuation-- so something something like re.split('(\\s|[\\.!\\?,;])', text)\n",
    "    is_structured is a list of same length as tokens, with booleans \n",
    "    representing whether corresponding token is part of structured info.\n",
    "    Possible outputs:\n",
    "    1. Start autocomplete of type X\n",
    "    2. Stop autocomplete\n",
    "    3. Continue previous autocomplete type\n",
    "    \"\"\"\n",
    "    prompt_autocomplete = False\n",
    "    autocomplete_type = None\n",
    "    whitelisted_tokens = set()\n",
    "    for i, (token, token_is_structured) in enumerate(zip(tokens, is_structured)):\n",
    "        if '(' in token:\n",
    "            continue \n",
    "        triggers = scope_trigger_trie.prefixes(unicode(''.join(tokens[i:])))\n",
    "        if triggers:\n",
    "            trigger = max(triggers, key=len)\n",
    "            prompt_autocomplete = True\n",
    "            autocomplete_type = scope_trigger_types[trigger]\n",
    "            for j in range(len(re.split('(\\s|[\\.!\\?,;])', trigger))):\n",
    "                whitelisted_tokens.add(j + i)\n",
    "            continue\n",
    "        if token_is_structured != -1:\n",
    "            prompt_autocomplete = True\n",
    "            autocomplete_type = term_trigger_types[token_is_structured]\n",
    "            continue\n",
    "        keep_scope = (i in whitelisted_tokens or (token_is_structured!=-1) or token in scope_continuation_tokens or bool(re.match('\\s', token)))\n",
    "        if not keep_scope:\n",
    "            prompt_autocomplete = False\n",
    "            autocomplete_type = None\n",
    "    return autocomplete_type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_omr(visit_ix):\n",
    "    # determine whether patient has any OMR  based on date of ED visit\n",
    "    visit = ed_visits_pkl[visit_ix]\n",
    "    md_comments = visit['MDcomments'][0]\n",
    "    visit_date = datetime.strptime(visit['Date'][0][14:24], '%Y-%m-%d')\n",
    "    pid = visit['PatientID'][0]\n",
    "    if not md_comments:\n",
    "        return None\n",
    "    triage_assessment = visit['TriageAssessment'][0]\n",
    "    omr_buckets, omr_terms = [], []\n",
    "    if pid in omr_data:\n",
    "        last_omr_note = 0\n",
    "        for i, note in enumerate(omr_data[pid]):\n",
    "            if note['time'] < visit_date:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "omr_date_parser = lambda x : datetime.strptime(x['time'], '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_autocompletes(visit_ix):\n",
    "    visit = ed_visits_pkl[visit_ix]\n",
    "    md_comments = visit['MDcomments'][0]\n",
    "    visit_date = datetime.strptime(visit['Date'][0][14:24], '%Y-%m-%d')\n",
    "    pid = visit['PatientID'][0]\n",
    "    if not md_comments:\n",
    "        return None, None, None, None\n",
    "    triage_assessment = visit['TriageAssessment'][0]\n",
    "    vitals = {\n",
    "        'Age' : visit['Age'][0],\n",
    "        'Temp' : visit['TriageTemp'][0],\n",
    "        'RR' : visit['TriageRR'][0],\n",
    "        'Pulse' : visit['TriageHR'][0],\n",
    "        'O2Sat' : visit['TriageSaO2'][0],\n",
    "        'BP' : visit['TriageBP'][0],\n",
    "        'Sex' : visit['Sex'][0],\n",
    "        'Acuity' : visit['TriageAcuity'][0][0] if visit['TriageAcuity'][0] else None \n",
    "    }\n",
    "    chief_complaint = list(ed_chief_complaints[visit_ix][-1])\n",
    "    chief_complaint = chief_complaint[0] if chief_complaint else None \n",
    "    hpi = md_comments.split('\\n')[0].lower()\n",
    "    tokens = [t for t in re.split('(\\s|[\\.!\\?,;])', hpi) if t != '']\n",
    "    concepts = find_concepts(tokens)\n",
    "    omr_buckets, omr_terms = [], []\n",
    "    if pid in omr_data:\n",
    "        last_omr_note = 0\n",
    "        for i, note in enumerate(omr_data[pid]):\n",
    "            if omr_date_parser(note) >= visit_date:\n",
    "                last_omr_note = i\n",
    "                break\n",
    "        omr_notes = [] if (last_omr_note==0) else omr_data[pid][:i]\n",
    "        omr_buckets, omr_terms = hpi_autocomplete.get_omr_buckets(omr_notes)\n",
    "        \n",
    "    # can be changed to get_freq_ranking and get_spell_ranking for others\n",
    "    lab_ranking = lab_autocomplete.get_frequency_ranking()\n",
    "    med_ranking = medication_autocomplete.get_frequency_ranking()\n",
    "    symptom_ranking = symptom_autocomplete.get_ranking(chief_complaint, vitals)  # can change to get_freq_ranking() or get_spell_ranking() for freq/spell baselines\n",
    "    hpi_ranking = hpi_autocomplete.get_ranking(triage_assessment, list(omr_buckets), omr_terms, None) # can change to get_freq_ranking() or get_spell_ranking() for freq and spell baselines\n",
    "    rankings = [\n",
    "        [('DISEASE', t) for t in hpi_ranking],\n",
    "        [('SYMPTOM', t) for t in symptom_ranking],\n",
    "        [('MEDICATION', t) for t in med_ranking],\n",
    "        [('LAB', t) for t in lab_ranking]\n",
    "    ]\n",
    "    type_map = {\n",
    "        'DISEASE': 0,\n",
    "        'SYMPTOM': 1,\n",
    "        'MEDICATION': 2,\n",
    "        'LAB': 3\n",
    "    }\n",
    "    is_structured = []\n",
    "    keystroke_metric = []\n",
    "    regular_metric = []\n",
    "    mrrs = []\n",
    "    scope_misses = []\n",
    "    type_misses = []\n",
    "    typed_text_list = []\n",
    "    in_omr_list = []\n",
    "    i, tok = 0, tokens[0]\n",
    "    while i  < len(tokens) - 1:\n",
    "        tok = tokens[i]\n",
    "        if i in concepts:\n",
    "            autocomplete_type = predict_autocomplete_scope(tokens[:i], is_structured) # predicted autocomplete type\n",
    "            if autocomplete_type is None:\n",
    "                scope_misses.append(1) # autocomplete type is \n",
    "                autocomplete_type = 'DISEASE'\n",
    "            else:\n",
    "                scope_misses.append(0)\n",
    "            relevant_text = concepts[i][1]\n",
    "            relevant_text_token_length = len(re.split('(\\s|[\\.!\\?,;])', relevant_text))\n",
    "            relevant_term = term_lookup[relevant_text]\n",
    "            relevant_term_type = relevant_term[0]\n",
    "            type_misses.append(int(relevant_term_type != autocomplete_type))\n",
    "            new_rankings = rankings[type_map[autocomplete_type]] + sum([\n",
    "                rankings[j] for j in range(4) if j != type_map[autocomplete_type]\n",
    "            ], [])\n",
    "            num_keystrokes = get_num_keystrokes_singlequery(new_rankings, relevant_term, relevant_text, k=3)\n",
    "            mrr = reciprocal_rank(rankings[type_map[relevant_term_type]], relevant_term, len(concepts))\n",
    "            mrrs.append(mrr)\n",
    "            keystroke_metric.append(num_keystrokes)\n",
    "            regular_metric.append(len(relevant_text))\n",
    "            typed_text_list.append(relevant_text)\n",
    "            rankings[type_map[relevant_term_type]].remove(relevant_term)\n",
    "            rankings[type_map[relevant_term_type]].append(relevant_term)\n",
    "            is_structured += [type_map[relevant_term_type] for _ in range(relevant_text_token_length)]\n",
    "            i += relevant_text_token_length\n",
    "            in_omr_list.append(relevant_term[1] in omr_terms)\n",
    "        else:\n",
    "            is_structured.append(-1) \n",
    "            i += 1\n",
    "    return regular_metric, keystroke_metric, scope_misses, type_misses, typed_text_list, in_omr_list, mrrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#freq_results = []\n",
    "#spell_results = []\n",
    "all_results = []\n",
    "for j, i in enumerate(test_indices):\n",
    "    all_results.append(run_all_autocompletes(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "keystrokes_per_concept = {}\n",
    "keystrokes_per_concept_omr = {}\n",
    "keystrokes_per_concept_no_prior = {}\n",
    "keystrokes_per_concept_prior_no_omr = {}\n",
    "mrr_per_concept_freq = {}\n",
    "mrr_per_concept_cont = {}\n",
    "k_burden = []\n",
    "for x, y, index in zip(freq_results, all_results, test_indices[:2000]):\n",
    "    omr_present = has_omr(index)\n",
    "    if len(x) != 7:\n",
    "        continue\n",
    "    words_spell = x[-3]\n",
    "    ks_spell = x[1]\n",
    "    words_all = y[-3]\n",
    "    ks_all = y[1]\n",
    "    for word, k_spell, _, k_all, in_omr, mrr_freq, mrr_cont in zip(words_spell, ks_spell, words_all, ks_all, x[-2], x[-1], y[-1]):\n",
    "        if word == 'as':\n",
    "            continue\n",
    "        term_type = term_lookup[word]\n",
    "        if term_type[0] != 'DISEASE':\n",
    "            continue\n",
    "        word_counts[term_type] = word_counts.get(term_type, 0) + 1\n",
    "        keystrokes_per_concept[term_type] = keystrokes_per_concept.get(term_type, []) + [k_spell - k_all]\n",
    "        mrr_per_concept_freq[term_type] = mrr_per_concept_freq.get(term_type, []) + [mrr_freq]\n",
    "        mrr_per_concept_cont[term_type] = mrr_per_concept_cont.get(term_type, []) + [mrr_cont]\n",
    "        if in_omr:\n",
    "            keystrokes_per_concept_omr[term_type] = keystrokes_per_concept_omr.get(term_type, []) + [k_spell - k_all]\n",
    "            k_burden.append(k_all)\n",
    "        if not omr_present:\n",
    "            keystrokes_per_concept_no_prior[term_type] = keystrokes_per_concept_no_prior.get(term_type, []) + [k_spell - k_all]\n",
    "        if omr_present and not in_omr:\n",
    "            keystrokes_per_concept_prior_no_omr[term_type] = keystrokes_per_concept_prior_no_omr.get(term_type, []) + [k_spell - k_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_freqs, mean_conts, conf_freqs, conf_conts, cnames = [], [], [], [], []\n",
    "for c in [2, 19, 23, 54, 153, 159, 223, 226]: # global interpretability case studies\n",
    "    common_name = hpi_ontology.loc[c]['common_name']\n",
    "    term_type = ('DISEASE', c)\n",
    "    mean_freq = np.mean(mrr_per_concept_freq[term_type])\n",
    "    conf_freq = conf_interval(mrr_per_concept_freq[term_type])\n",
    "    mean_cont = np.mean(mrr_per_concept_cont[term_type])\n",
    "    conf_cont = conf_interval(mrr_per_concept_cont[term_type])\n",
    "    cnames.append(common_name)\n",
    "    mean_freqs.append(mean_freq)\n",
    "    mean_conts.append(mean_cont)\n",
    "    conf_freqs.append(mean_freq - conf_freq[0])\n",
    "    conf_conts.append(mean_cont - conf_cont[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(np.arange(len(mean_conts)), mean_conts, 0.3, xerr = conf_conts, alpha=0.8, label='Contextual', capsize=3, color='purple')\n",
    "plt.barh(np.arange(len(mean_conts)) + 0.3, mean_freqs, 0.3, xerr = conf_freqs, alpha=0.8, label='Frequency', capsize=3, color='orange')\n",
    "plt.legend()\n",
    "plt.xlim(0, 1.4)\n",
    "plt.yticks(np.arange(len(mean_conts)), cnames);\n",
    "plt.xlabel('MRR')\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(7, 7)\n",
    "plt.savefig('mrr_by_concept.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_per_concept_freq = { k : np.mean(v) for k, v in mrr_per_concept_freq.items()}\n",
    "mrr_per_concept_cont = { k : np.mean(v) for k, v in mrr_per_concept_cont.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jclinic_py2]",
   "language": "python",
   "name": "conda-env-jclinic_py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
